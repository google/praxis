{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSjttCS1tmbH"
      },
      "source": [
        "This colab is an attempt to implement SEDD: Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution\n",
        "\n",
        "Arxiv paper is https://arxiv.org/pdf/2310.16834"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0ysFB03tX_S"
      },
      "outputs": [],
      "source": [
        "from jax import numpy as jnp\n",
        "# pylint: disable=g-multiple-import, g-importing-member\n",
        "from jaxtyping import Array, Float, Int32\n",
        "from typing import Protocol\n",
        "\n",
        "import jax\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u6rxqAc94iD"
      },
      "source": [
        "# Implement key algorithmic pieces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upyzrj2H-FeX"
      },
      "source": [
        "## Diffusion intensity\n",
        "Here we implement objects that governs how diffusion intensity changes over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBoojyN8tmAG"
      },
      "outputs": [],
      "source": [
        "# Start with some basics.\n",
        "FloatNdArrayOrSclar = Float[Array, 'B ...'] | Float\n",
        "IntNdArray = Int32[Array, 'B ...']\n",
        "\n",
        "\n",
        "class DiffusionIntenistyInterface(Protocol):\n",
        "  \"\"\"Protocol encapsulating how diffusion intensity changes over time.\n",
        "\n",
        "  Unlike in the paper, here we always assume time starts at 0 and ends in 1.\n",
        "\n",
        "  In the forward process, an example x_0 sampled from the data distribution\n",
        "  p_data undergoes the discrete diffusion process to gradually becomes complete\n",
        "  noise.\n",
        "\n",
        "  In the backward process, we gradually denoise the noisy sample to uncover the\n",
        "  original example.\n",
        "  \"\"\"\n",
        "\n",
        "  def sigma_t(self, time: FloatNdArrayOrSclar) -\u003e FloatNdArrayOrSclar:\n",
        "    # This corresponds to \\sigma(t) in the paper, indicating the rate of change\n",
        "    # at a particular point of time in diffusion probabilities.\n",
        "    ...\n",
        "\n",
        "  def cum_sigma_t(self, time: FloatNdArrayOrSclar) -\u003e FloatNdArrayOrSclar:\n",
        "    # This corresponds to \\bar{\\sigma}(t) in the paper. It is the cumulative of\n",
        "    # sigma_t from time 0.\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHpWDBIcNfO-"
      },
      "outputs": [],
      "source": [
        "class LinearDiffusionDensity(DiffusionIntenistyInterface):\n",
        "\n",
        "  def __init__(self, strength = 1.0):\n",
        "    self._strength = strength\n",
        "\n",
        "  def sigma_t(self, time: FloatNdArrayOrSclar) -\u003e FloatNdArrayOrSclar:\n",
        "    return jnp.ones_like(time) * self._strength\n",
        "\n",
        "  def cum_sigma_t(self, time: FloatNdArrayOrSclar) -\u003e FloatNdArrayOrSclar:\n",
        "    # clamp time to be between 0 and 1.\n",
        "    time = jnp.maximum(jnp.minimum(time, 1.0), 0.0)\n",
        "    return time * self._strength"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8XUYVzc-aXq"
      },
      "source": [
        "### unit-tests for diffusion intensity objects\n",
        "Add a bunch of tests. TODO(yonghui): convert them to unit-tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfWL9uD2OHdb"
      },
      "outputs": [],
      "source": [
        "# Test the implementation of LinearDiffusionDensity.\n",
        "x = LinearDiffusionDensity()\n",
        "time = np.array([-1.0, 0.2, 0.7, 1.2])\n",
        "# The expected output is [1, 1, 1, 1]\n",
        "print(x.sigma_t(time))\n",
        "# The expected output is [0, 0.2, 0.7, 1.0]\n",
        "print(x.cum_sigma_t(time))\n",
        "\n",
        "time = 0.5\n",
        "print(x.sigma_t(time))\n",
        "print(x.cum_sigma_t(time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yed8ZUjv-hLY"
      },
      "source": [
        "## Diffusion Matrices\n",
        "\n",
        "They governs the diffusion process, how probability mass flows from one node to othes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5xNNKpvwAYR"
      },
      "outputs": [],
      "source": [
        "# Transition matrix is a key concept used in discrete diffusion process, It\n",
        "# gonverns the diffusion process.\n",
        "class TransitionMatrix(Protocol):\n",
        "  \"\"\"Protocol defines the transition matrix Q.\n",
        "\n",
        "  A transition matrix, denoted as Q in the paper, is of size\n",
        "  [vocab_size, vocab_size], where vocab_size is the size of the vocab.\n",
        "\n",
        "  The semantics of Q(i, j) is the following: given X_t = j, the\n",
        "  probability of X_{t+\\delta t} = i equals Q(j, i) * \\delta t.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, rank: int, diff_density: DiffusionIntenistyInterface):\n",
        "    # The transition matrix is of size rank * rank.\n",
        "    self._rank = rank\n",
        "    # 'diff_density' governs how diffusion density changes over time.\n",
        "    self._diff_density = diff_density\n",
        "\n",
        "  def q(self) -\u003e Float[Array, 'N N']:\n",
        "    \"\"\"Returns the Q matrix. This is only useful for testing purposes.\"\"\"\n",
        "    cols = []\n",
        "    for i in range(self._rank):\n",
        "      cols.append(self.q_column(i))\n",
        "    # Now stack them together\n",
        "    return jnp.stack(cols, axis=1)\n",
        "\n",
        "  def q_column(self, col: int) -\u003e Float[Array, 'N']:\n",
        "    \"\"\"Returns 'col'-th column vector of the Q matrix, of dim 'self._rank'.\n",
        "\n",
        "    Args:\n",
        "      col: the column to return.\n",
        "\n",
        "    Returns:\n",
        "      A vector of dim 'self._rank'.\n",
        "\n",
        "    The returned vector should sum up to 0.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def q_row(self, row: int) -\u003e Float[Array, 'N']:\n",
        "    \"\"\"Returns 'row'-th row vector of the Q matrix, of dim 'self._rank'.\n",
        "\n",
        "    Args:\n",
        "      row: the row to return.\n",
        "\n",
        "    Returns:\n",
        "      A vector of dim 'self._rank'.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def batch_q_row(self, rows: Int32[Array, 'B']) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"The batched version of q_row for more efficient training / sampling.\"\"\"\n",
        "    ...\n",
        "\n",
        "  def q_posterior(\n",
        "      self, cols: Int32[Array, 'B'], prob_ratio: Float[Array, 'B N']\n",
        "  ) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"Returns the posterior transition matrix.\n",
        "\n",
        "    q_posterior(i, j) = q(j, i) * p(i) / p(j)\n",
        "\n",
        "    This is easy to see:\n",
        "    p(i | j) = p(j | i) * p(i) / p(j)\n",
        "\n",
        "    In the context of discrete diffusion process:\n",
        "    p(x_{t - \\delta t} | x_t) = p(x_t | x_{t - \\delta t}) *\n",
        "                                p(x_{t - \\delta t}) / p(x_t)\n",
        "    where p(x_t | x_{t - \\delta t}) is the forward transition matrix.\n",
        "\n",
        "    Args:\n",
        "      cols: is a vector of indices of the node x_t. It is also the column\n",
        "        indices into the posterior matrix.\n",
        "      prob_ratio: prob ratio of all other nodes wrt to node x_t\n",
        "\n",
        "    Returns:\n",
        "      A matrix of transition probs from node x_t to x_{t - \\delta t}.\n",
        "\n",
        "    Please refer to equation 3 in the paper for details.\n",
        "    \"\"\"\n",
        "    # of shape [B, N]\n",
        "    post_p = self.batch_q_row(cols) * prob_ratio\n",
        "    # masks out x_t themselves.\n",
        "    mask = 1.0 - jax.nn.one_hot(cols, self._rank)\n",
        "    post_p = post_p * mask\n",
        "    # of shape [B, 1]\n",
        "    post_p_sum = jnp.sum(post_p, axis=1, keepdims=True)\n",
        "    return post_p - post_p_sum * (1 - mask)\n",
        "\n",
        "  def exp_q(self, node: int, time: float) -\u003e Float[Array, 'N']:\n",
        "    \"\"\"Returns the cumulative transition prob from 'node' to all others nodes.\n",
        "\n",
        "    Args:\n",
        "      node: the index (into the vocab) of the current node. This is the state of\n",
        "        the variable at time 0.\n",
        "      time: the time at which we want to compute the cumulative transition prob\n",
        "        for.\n",
        "\n",
        "    Returns:\n",
        "      A float array of the transition probs, of size [N], where N is the size of\n",
        "      the vocab. Given X_0 == node (the state of the random variable at time 0),\n",
        "      this is the marginal distribution of X_t at time 'time'. The returned\n",
        "      vector should sum up to 1.0.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def batch_exp_q(\n",
        "      self, nodes: Int32[Array, 'B'], time: Float[Array, 'B']\n",
        "  ) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"A batched version of exp_q above.\n",
        "\n",
        "    Args:\n",
        "      nodes: a vector of [B] of the current nodes\n",
        "      time: a vector of [B] of the times for which to compute the cumulative\n",
        "        transition probs for. 'time' can be different for different nodes.\n",
        "\n",
        "    Returns:\n",
        "      A matrix of size [B, N], where B is the number of nodes, and N is the\n",
        "      vocab size.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def batch_exp_q_row(\n",
        "      self, nodes: Int32[Array, 'B'], time: Float[Array, 'B']\n",
        "  ) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"useful for sampling\n",
        "\n",
        "    Conceptually, what this function does is the following\n",
        "\n",
        "    rows = []\n",
        "    for node, time in zip(nodes, times):\n",
        "      # compute the comulative transition prob from time 0 to time t\n",
        "      exp_q = exp(t * self.q)\n",
        "      rows.append(exp_q[node])\n",
        "    return jnp.stack(rows)\n",
        "\n",
        "    Args:\n",
        "      nodes: a vector of [B] of the current nodes\n",
        "      time: a vector of [B] of the times for which to compute the cumulative\n",
        "        transition probs for. 'time' can be different for different nodes.\n",
        "\n",
        "    Returns:\n",
        "      A matrix of size [B, N], where B is the number of nodes, and N is the\n",
        "      vocab size. See above for the semantics of the matrix.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def adjust_prob_ratio(\n",
        "      self, prob_ratio: Float[Array, 'B N'], delta_ts: Float[Array, 'B']\n",
        "  ) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"Given prob ratios at time 'ts', we estimate what they would have been at\n",
        "    time 'ts - delta_t'.\n",
        "\n",
        "    Mathmatically, here is what we compute:\n",
        "\n",
        "    given:\n",
        "      p_t = P_{transition} p_0\n",
        "\n",
        "    we would like to compute:\n",
        "      p_0 = P_{transition}^(-1) p_t\n",
        "\n",
        "    where p_0 and p_t could be probabilities or probability ratios.\n",
        "\n",
        "    Args:\n",
        "      prob_ratio: the probability ratio at time t\n",
        "      delta_ts: we would like to estimate the probability ratio at time t -\n",
        "        delta_ts\n",
        "\n",
        "    Returns:\n",
        "      Probability or prob ratio at time t - delta_ts\n",
        "    \"\"\"\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVqTzI3s0ZZn"
      },
      "outputs": [],
      "source": [
        "class QAbsorb(TransitionMatrix):\n",
        "  \"\"\"A transition matrix that gradually diffuses any distribution into a ...\n",
        "\n",
        "  distribution centered around the sink node.\n",
        "  \"\"\"\n",
        "\n",
        "  def q_column(self, col: int) -\u003e Float[Array, 'N']:\n",
        "    \"\"\"Returns the 'col'-th column vector of the Q matrix, of dim 'self._rank'.\n",
        "\n",
        "    Args:\n",
        "      col: the column to return.\n",
        "\n",
        "    Returns:\n",
        "      A vector of dim 'self._rank'.\n",
        "\n",
        "    The returned vector should sum up to 0.\n",
        "    \"\"\"\n",
        "    # probability mass flows out of the current node.\n",
        "    cur_node = -1.0 * jax.nn.one_hot(col, self._rank)\n",
        "    # probability mass flows into the sink node. The sink node is always at\n",
        "    # the end (the last entry in the vocab).\n",
        "    sink_node = 1.0 * jax.nn.one_hot(self._rank - 1, self._rank)\n",
        "    return cur_node + sink_node\n",
        "\n",
        "  def q_row(self, row: int) -\u003e Float[Array, 'N']:\n",
        "    \"\"\"Returns 'row'-th row vector of the Q matrix, of dim 'self._rank'.\n",
        "\n",
        "    Args:\n",
        "      row: the row to return.\n",
        "\n",
        "    Returns:\n",
        "      A vector of dim 'self._rank'.\n",
        "    \"\"\"\n",
        "    row_vec_normal = -1.0 * jax.nn.one_hot(row, self._rank)\n",
        "    row_vec_sink = jnp.ones(shape=[self._rank]) + row_vec_normal\n",
        "    return jnp.where(row \u003c self._rank - 1, row_vec_normal, row_vec_sink)\n",
        "\n",
        "  def batch_q_row(self, rows: Int32[Array, 'B']) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"The batched version of q_row for more efficient training / sampling.\"\"\"\n",
        "    assert rows.ndim == 1\n",
        "    batch_size = rows.shape[0]\n",
        "    # of shape [B, N]\n",
        "    row_vec_normal = -1.0 * jax.nn.one_hot(rows, self._rank)\n",
        "    row_vec_sink = jnp.ones(shape=[batch_size, self._rank]) + row_vec_normal\n",
        "    # Now choose between row_vec_normal and row_vec_sink based on the node_id.\n",
        "    return jnp.where(\n",
        "        rows[:, jnp.newaxis] \u003c self._rank - 1, row_vec_normal, row_vec_sink\n",
        "    )\n",
        "\n",
        "  def exp_q(self, node: int, time: float) -\u003e Float[Array, 'N']:\n",
        "    \"\"\"Returns the cumulative transition probability from 'node' to others.\n",
        "\n",
        "    Args:\n",
        "      node: the index (into the vocab) of the current node.\n",
        "      time: the time at which we want to compute the cumulative transition prob\n",
        "        for.\n",
        "\n",
        "    Returns:\n",
        "      A float array of the transition probs, of size [N], where N is the size of\n",
        "      the vocab. Given X_0 == node (the state of the random variable at time 0),\n",
        "      this is the marginal distribution of X_t at time 'time'. The returned\n",
        "      vector should sum up to 1.0.\n",
        "    \"\"\"\n",
        "    # Prob mass that still remains at the current node.\n",
        "    node_prob = jnp.exp(-1.0 * self._diff_density.cum_sigma_t(time))\n",
        "    sink_prob = 1.0 - node_prob\n",
        "    # This is the probability mass that still remain at the current node.\n",
        "    cur_node = node_prob * jax.nn.one_hot(node, self._rank)\n",
        "    # This is the probability mass that is flowing into the sink node.\n",
        "    sink_node = sink_prob * jax.nn.one_hot(self._rank - 1, self._rank)\n",
        "    return cur_node + sink_node\n",
        "\n",
        "  def batch_exp_q(\n",
        "      self, nodes: Int32[Array, 'B'], time: Float[Array, 'B']\n",
        "  ) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"A batched version of exp_q above.\n",
        "\n",
        "    Args:\n",
        "      nodes: a vector of [B] of the current nodes\n",
        "      time: a vector of [B] of the times for which to return the cumulative\n",
        "        transition probs for. 'time' can be different for different nodes.\n",
        "\n",
        "    Returns:\n",
        "      A matrix of size [B, N], where B is the number of nodes, and N is the\n",
        "      vocab size.\n",
        "    \"\"\"\n",
        "    assert nodes.ndim == 1\n",
        "    assert time.ndim == 1\n",
        "    assert nodes.shape == time.shape\n",
        "    batch_size = nodes.shape[0]\n",
        "\n",
        "    # Prob mass that still remains at the current node.\n",
        "    node_prob = jnp.exp(-1.0 * self._diff_density.cum_sigma_t(time))\n",
        "    # expand node_prob into a matrix of shape [B, 1]\n",
        "    node_prob = node_prob[:, jnp.newaxis]\n",
        "    sink_prob = 1.0 - node_prob\n",
        "    # This is the probability mass that still remain at the current node.\n",
        "    cur_node = node_prob * jax.nn.one_hot(nodes, self._rank)\n",
        "    # This is the probability mass that is flowing into the sink node.\n",
        "    sink_node = sink_prob * jax.nn.one_hot(\n",
        "        jnp.zeros([batch_size], dtype=jnp.int32) + self._rank - 1, self._rank\n",
        "    )\n",
        "    return cur_node + sink_node\n",
        "\n",
        "  def batch_exp_q_row(\n",
        "      self, nodes: Int32[Array, 'B'], time: Float[Array, 'B']\n",
        "  ) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"Computes and returns the following:\n",
        "\n",
        "    rows = []\n",
        "    for node, time in zip(nodes, times):\n",
        "      # compute the comulative transition prob from time 0 to time t\n",
        "      exp_q = exp(t * self.q)\n",
        "      rows.append(exp_q[node])\n",
        "    return jnp.stack(rows)\n",
        "\n",
        "    Args:\n",
        "      nodes: a vector of [B] of the current nodes\n",
        "      time: a vector of [B] of the times for which to compute the cumulative\n",
        "        transition probs for. 'time' can be different for different nodes.\n",
        "\n",
        "    Returns:\n",
        "      A matrix of size [B, N], where B is the number of nodes, and N is the\n",
        "      vocab size. See above for the semantics of the matrix.\n",
        "    \"\"\"\n",
        "    assert nodes.ndim == 1\n",
        "    assert time.ndim == 1\n",
        "    assert nodes.shape == time.shape\n",
        "    batch_size = nodes.shape[0]\n",
        "\n",
        "    # Prob mass that still remains at the current node.\n",
        "    node_prob = jnp.exp(-1.0 * self._diff_density.cum_sigma_t(time))\n",
        "    # expand node_prob into a matrix of shape [B, 1]\n",
        "    node_prob = node_prob[:, jnp.newaxis]\n",
        "    # of shape [B, 1].\n",
        "    sink_prob = 1.0 - node_prob\n",
        "    # This is the probability mass that still remain at the current node.\n",
        "    row_vec_normal = node_prob * jax.nn.one_hot(nodes, self._rank)\n",
        "    sink_nodes = jnp.zeros([batch_size], dtype=jnp.int32) + self._rank - 1\n",
        "    row_vec_sink = sink_prob * jnp.ones(\n",
        "        shape=[batch_size, self._rank]\n",
        "    ) + row_vec_normal * jax.nn.one_hot(\n",
        "        sink_nodes, self._rank\n",
        "    )  # To make sure prob for the last entry is 1.0\n",
        "\n",
        "    return jnp.where(\n",
        "        nodes[:, jnp.newaxis] \u003c self._rank - 1, row_vec_normal, row_vec_sink\n",
        "    )\n",
        "\n",
        "  def adjust_prob_ratio(\n",
        "      self, prob_ratio: Float[Array, 'B N'], delta_ts: Float[Array, 'B']\n",
        "  ) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"Adjust prob ratio for QAbsorb.\n",
        "\n",
        "    Given the forward prob transition matrix of this form (this is the exp(tQ))\n",
        "    The backward transition matrix is of this form (this is exp(-tQ))\n",
        "    see https://chatgpt.com/share/67928ded-f5cc-800f-a944-caa36da2e830\n",
        "\n",
        "    Note naive implementation of\n",
        "    p_0 = Exp(-tQ) p_t would have time complexity of O(N^2), which is\n",
        "    prohibitively high for large N (e.g. 256k for large vocabs)\n",
        "\n",
        "    Here we take advantage of the fact that exp(-tQ) matrix is very sparse to\n",
        "    significantly cut-down the computation cost.\n",
        "    \"\"\"\n",
        "    # The forward transition probability on the diagnal.\n",
        "    #\n",
        "    # shape [B, 1]\n",
        "    one_over_q_x = jnp.exp(self._diff_density.cum_sigma_t(delta_ts))[\n",
        "        :, jnp.newaxis\n",
        "    ]\n",
        "    batch_size = prob_ratio.shape[0]\n",
        "    rank = prob_ratio.shape[1]\n",
        "    # shape [B N]\n",
        "    p_0_normal_node = prob_ratio * one_over_q_x\n",
        "    # of shape [B N]\n",
        "    #\n",
        "    exp_q_inverse_last_row = jnp.zeros([batch_size, rank]) + 1 - one_over_q_x\n",
        "    adjustment = (\n",
        "        jax.nn.one_hot(\n",
        "            jnp.zeros([batch_size], dtype=jnp.int32) + rank - 1, rank\n",
        "        )\n",
        "        * one_over_q_x\n",
        "    )\n",
        "    exp_q_inverse_last_row = exp_q_inverse_last_row + adjustment\n",
        "    # of shape [B]\n",
        "    p_0_sink_node = jnp.sum(exp_q_inverse_last_row * prob_ratio, axis=1)\n",
        "    # Finally, combine the two\n",
        "    # of shape [B, N]\n",
        "    sink_nodes = jax.nn.one_hot(\n",
        "        jnp.zeros([batch_size], dtype=jnp.int32) + rank - 1, rank\n",
        "    )\n",
        "    return (\n",
        "        p_0_normal_node * (1 - sink_nodes)\n",
        "        + p_0_sink_node[:, jnp.newaxis] * sink_nodes\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX6lvDfH3veG"
      },
      "outputs": [],
      "source": [
        "class QUniform(TransitionMatrix):\n",
        "  \"\"\"A transition matrix that diffuses into other nodes uniformly.\"\"\"\n",
        "\n",
        "  def q_column(self, col: int) -\u003e Float[Array, 'N']:\n",
        "    \"\"\"Returns 'col'-th column vector of the Q matrix, of dim 'self._rank'.\n",
        "\n",
        "    Args:\n",
        "      col: the column to return.\n",
        "\n",
        "    Returns:\n",
        "      A vector of dim 'self._rank'.\n",
        "\n",
        "    The returned vector should sum up to 0.\n",
        "    \"\"\"\n",
        "    # Probability mass flows out of the current node at uniform speed.\n",
        "    cur_node = -1.0 * jax.nn.one_hot(col, self._rank)\n",
        "    # Probability mass flows into other nodes at equal prob.\n",
        "    other_nodes = jnp.ones([self._rank]) / self._rank\n",
        "    return cur_node + other_nodes\n",
        "\n",
        "  def q_row(self, row: int) -\u003e Float[Array, 'N']:\n",
        "    \"\"\"Returns 'row'-th row vector of the Q matrix, of dim 'self._rank'.\n",
        "\n",
        "    Args:\n",
        "      row: the row to return.\n",
        "\n",
        "    Returns:\n",
        "      A vector of dim 'self._rank'.\n",
        "    \"\"\"\n",
        "    # The uniform transition matrix is doubly stochastic\n",
        "    return self.q_column(row)\n",
        "\n",
        "  def batch_q_row(self, rows: Int32[Array, 'B']) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"The batched version of q_row for more efficient training / sampling.\"\"\"\n",
        "    # UniformDiffusion matrix is doublely stochastic. Q == Q^T\n",
        "    assert rows.ndim == 1\n",
        "    batch_size = rows.shape[0]\n",
        "    #\n",
        "    # Of shape [B, N]\n",
        "    cur_node = -1.0 * jax.nn.one_hot(rows, self._rank)\n",
        "    # Probability mass flows into other nodes at equal prob.\n",
        "    other_nodes = jnp.ones([batch_size, self._rank]) / self._rank\n",
        "    return cur_node + other_nodes\n",
        "\n",
        "  def exp_q(self, node: int, time: float) -\u003e Float[Array, 'N']:\n",
        "    \"\"\"Returns the cumulative transition probability from 'node' to others.\n",
        "\n",
        "    Args:\n",
        "      node: the index (into the vocab) of the current node.\n",
        "      time: the time at which we want to compute the cumulative transition prob\n",
        "        for.\n",
        "\n",
        "    Returns:\n",
        "      A float array of the transition probs, of size [N], where N is the size of\n",
        "      the vocab. Given X_0 == node (the state of the random variable at time 0),\n",
        "      this is the marginal distribution of X_t at time 'time'. The returned\n",
        "      vector should sum up to 1.0.\n",
        "    \"\"\"\n",
        "    # prob mass still remain at the current node\n",
        "    node_prob = jnp.exp(-1.0 * self._diff_density.cum_sigma_t(time))\n",
        "    other_prob = (1.0 - node_prob) / self._rank\n",
        "    cur_node = node_prob * jax.nn.one_hot(node, self._rank)\n",
        "    other_nodes = other_prob * jnp.ones([self._rank])\n",
        "    return cur_node + other_nodes\n",
        "\n",
        "  def batch_exp_q(\n",
        "      self, nodes: Int32[Array, 'B'], time: Float[Array, 'B']\n",
        "  ) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"A batched version of exp_q above.\n",
        "\n",
        "    Args:\n",
        "      nodes: a vector of [B] of the current nodes\n",
        "      time: a vector of [B] of the times for which to return the cumulative\n",
        "        transition probs for. 'time' can be different for different nodes.\n",
        "\n",
        "    Returns:\n",
        "      A matrix of size [B, N], where B is the number of nodes, and N is the\n",
        "      vocab size.\n",
        "    \"\"\"\n",
        "    assert nodes.ndim == 1\n",
        "    assert time.ndim == 1\n",
        "    assert nodes.shape == time.shape\n",
        "    batch_size = nodes.shape[0]\n",
        "    # prob mass still remain at the current node\n",
        "    node_prob = jnp.exp(-1.0 * self._diff_density.cum_sigma_t(time))\n",
        "    other_prob = (1.0 - node_prob) / self._rank\n",
        "    node_prob = node_prob[:, jnp.newaxis]\n",
        "    other_prob = other_prob[:, jnp.newaxis]\n",
        "    cur_node = node_prob * jax.nn.one_hot(nodes, self._rank)\n",
        "    other_nodes = other_prob * jnp.ones([batch_size, self._rank])\n",
        "    return cur_node + other_nodes\n",
        "\n",
        "  def batch_exp_q_row(\n",
        "      self, nodes: Int32[Array, 'B'], time: Float[Array, 'B']\n",
        "  ) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"Computes and returns the following:\n",
        "\n",
        "    rows = []\n",
        "    for node, time in zip(nodes, times):\n",
        "      # compute the comulative transition prob from time 0 to time t\n",
        "      exp_q = exp(t * self.q)\n",
        "      rows.append(exp_q[node])\n",
        "    return jnp.stack(rows)\n",
        "\n",
        "    Args:\n",
        "      nodes: a vector of [B] of the current nodes\n",
        "      time: a vector of [B] of the times for which to compute the cumulative\n",
        "        transition probs for. 'time' can be different for different nodes.\n",
        "\n",
        "    Returns:\n",
        "      A matrix of size [B, N], where B is the number of nodes, and N is the\n",
        "      vocab size. See above for the semantics of the matrix.\n",
        "    \"\"\"\n",
        "    assert nodes.ndim == 1\n",
        "    assert time.ndim == 1\n",
        "    assert nodes.shape == time.shape\n",
        "    # Since uniform distribution is doublely stochastic, batch_exp_q_row is\n",
        "    # the same as batch_exp_q_column.\n",
        "    return self.batch_exp_q(nodes, time)\n",
        "\n",
        "  def adjust_prob_ratio(\n",
        "      self, prob_ratio: Float[Array, 'B N'], delta_ts: Float[Array, 'B']\n",
        "  ) -\u003e Float[Array, 'B N']:\n",
        "    \"\"\"Adjust prob ratio for QUniform.\n",
        "\n",
        "    Given the forward prob transition matrix of this form (this is the exp(tQ)),\n",
        "    The backward transition matrix is of this form (this is exp(-tQ)):\n",
        "    See https://chatgpt.com/share/67933f48-51ec-800f-9088-bf7dfd3aa723 for the\n",
        "    derivation.\n",
        "\n",
        "    Note naive implementation of\n",
        "    p_0 = Exp(-tQ) p_t would have time complexity of O(N^2), which is\n",
        "    prohibitively high for large N (e.g. 256k for large vocab)\n",
        "\n",
        "    Here we take advantage of the structural property of the exp(-tQ) matrix\n",
        "    to reduce the time complexity to O(N).\n",
        "    \"\"\"\n",
        "    # The following is an implementation of the analytical solution above.\n",
        "    rank = prob_ratio.shape[1]\n",
        "    # (1 - x) and x as in https://chatgpt.com/share/67933f48-51ec-800f-9088-bf7dfd3aa723\n",
        "    one_minus_x = jnp.exp(-1.0 * self._diff_density.cum_sigma_t(delta_ts))\n",
        "    one_minus_x = one_minus_x[:, jnp.newaxis]\n",
        "    x = (1.0 - one_minus_x)\n",
        "    # of shape [B, 1]\n",
        "    j_matrix = (-1.0 * x) / (rank * one_minus_x)\n",
        "    # of shape [B, 1]\n",
        "    i_matrix = rank / (rank * one_minus_x)\n",
        "    # i * p_t\n",
        "    i_pt = i_matrix * prob_ratio\n",
        "    # j * p_t\n",
        "    j_pt = jnp.sum(j_matrix * prob_ratio, axis=1, keepdims=True)\n",
        "    return i_pt + j_pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-JkvZkQ-1rh"
      },
      "source": [
        "### unit-tests for diffusion matrices\n",
        "Add a bunch of unit-tests to make sure the diffusion matrices are correctly implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veFdu7QAPoNJ"
      },
      "outputs": [],
      "source": [
        "# Test the implementation of diffusion matrices.\n",
        "diff_density = LinearDiffusionDensity()\n",
        "rank = 8\n",
        "q_absorb = QAbsorb(rank, diff_density)\n",
        "print(q_absorb.q_column(0))\n",
        "print(q_absorb.q_column(2))\n",
        "print(q_absorb.q_column(rank - 1))\n",
        "print(q_absorb.exp_q(1, time=0.5))\n",
        "print(\n",
        "    q_absorb.batch_exp_q(\n",
        "        jnp.array([1, 2, rank - 1]), time=jnp.array([0.5, 0.75, 1.0])\n",
        "    )\n",
        ")\n",
        "\n",
        "q_uniform = QUniform(rank, diff_density)\n",
        "print(q_uniform.q_column(0))\n",
        "print(q_uniform.q_column(2))\n",
        "print(q_uniform.q_column(rank - 1))\n",
        "print(q_uniform.exp_q(1, time=0.5))\n",
        "print(\n",
        "    q_uniform.batch_exp_q(\n",
        "        jnp.array([1, 2, rank - 1]), time=jnp.array([0.5, 0.75, 1.0])\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw7DgCSvpIfq"
      },
      "outputs": [],
      "source": [
        "# TODO(yonghui): Convert this to a unit-test.\n",
        "# TODO(yonghui): Add more unit-test for the corner cases.\n",
        "#\n",
        "# Test the consistency between q_column and exp_q(). exp_q() should be a time\n",
        "# of q_column.\n",
        "#\n",
        "# Here, we test the equivallence of the QAbsorb matrix.\n",
        "diff_density = LinearDiffusionDensity(strength=2.0)\n",
        "rank = 8\n",
        "q_absorb = QAbsorb(rank, diff_density)\n",
        "\n",
        "# Assume the starting state is x_0\n",
        "x_0 = 2\n",
        "t_0 = 0.0\n",
        "t_end = 0.5\n",
        "q_matrix = q_absorb.q()\n",
        "q_matrix = jnp.astype(q_matrix, jnp.float64)\n",
        "print('q_matrix', q_matrix)\n",
        "\n",
        "x_t_end = q_absorb.exp_q(x_0, time=t_end)\n",
        "print('x_t', x_t_end)\n",
        "\n",
        "# now we integrate q_column from time t_0 to t_end\n",
        "x_t = jax.nn.one_hot(x_0, rank, dtype=jnp.float64)\n",
        "x_t = x_t[:, jnp.newaxis]\n",
        "print(x_t)\n",
        "num_iterations = 1000\n",
        "t_bucket_size = (t_end - t_0) / num_iterations\n",
        "for i in range(num_iterations):\n",
        "  t_i_begin = i * t_bucket_size\n",
        "  t_i_end = (i + 1) * t_bucket_size\n",
        "  delta_t = diff_density.cum_sigma_t(t_i_end) - diff_density.cum_sigma_t(\n",
        "      t_i_begin)\n",
        "  # This follows the differential equation.\n",
        "  x_t = x_t + jnp.matmul(q_matrix, x_t) * delta_t\n",
        "x_t_end_integrated = x_t\n",
        "x_t_end_integrated = jnp.reshape(x_t_end_integrated, [-1])\n",
        "print('x_t_end_integrated', x_t_end_integrated)\n",
        "assert np.max(np.abs(x_t_end_integrated - x_t_end)) \u003c 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-KA1n3LuMn5"
      },
      "outputs": [],
      "source": [
        "# TODO(yonghui): Convert this to a unit-test.\n",
        "# TODO(yonghui): Add more tests for the corner cases.\n",
        "#\n",
        "# Test the consistency between q_column and exp_q(). exp_q() should be a time\n",
        "# of q_column.\n",
        "#\n",
        "# Here, we test the equivallence of the QUniform matrix.\n",
        "diff_density = LinearDiffusionDensity(strength=2.0)\n",
        "rank = 8\n",
        "q_uniform = QUniform(rank, diff_density)\n",
        "\n",
        "# Assume the starting state is x_0\n",
        "x_0 = 2\n",
        "t_0 = 0.0\n",
        "t_end = 0.5\n",
        "q_matrix = q_uniform.q()\n",
        "q_matrix = jnp.astype(q_matrix, jnp.float64)\n",
        "print('q_matrix', q_matrix)\n",
        "\n",
        "x_t_end = q_uniform.exp_q(x_0, time=t_end)\n",
        "print('x_t', x_t_end)\n",
        "\n",
        "# now we integrate q_column from time t_0 to t_end\n",
        "x_t = jax.nn.one_hot(x_0, rank, dtype=jnp.float64)\n",
        "x_t = x_t[:, jnp.newaxis]\n",
        "print(x_t)\n",
        "num_iterations = 1000\n",
        "t_bucket_size = (t_end - t_0) / num_iterations\n",
        "for i in range(num_iterations):\n",
        "  t_i_begin = i * t_bucket_size\n",
        "  t_i_end = (i + 1) * t_bucket_size\n",
        "  delta_t = diff_density.cum_sigma_t(t_i_end) - diff_density.cum_sigma_t(\n",
        "      t_i_begin)\n",
        "  # This follows the differential equation.\n",
        "  x_t = x_t + jnp.matmul(q_matrix, x_t) * delta_t\n",
        "x_t_end_integrated = x_t\n",
        "x_t_end_integrated = jnp.reshape(x_t_end_integrated, [-1])\n",
        "print('x_t_end_integrated', x_t_end_integrated)\n",
        "assert np.max(np.abs(x_t_end_integrated - x_t_end)) \u003c 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMdwa2rEfxkU"
      },
      "outputs": [],
      "source": [
        "# Test the equivallence between batched and non-batched version.\n",
        "diff_density = LinearDiffusionDensity()\n",
        "rank = 8\n",
        "q_absorb = QAbsorb(rank, diff_density)\n",
        "nodes = [1, 2, rank - 1]\n",
        "ts = [0.5, 0.75, 1.0]\n",
        "\n",
        "xt_lists = []\n",
        "\n",
        "for node, t in zip(nodes, ts):\n",
        "  xt_lists.append(q_absorb.exp_q(node, time=t))\n",
        "xt_non_batch = jnp.stack(xt_lists, axis=0)\n",
        "\n",
        "xt_batch = q_absorb.batch_exp_q(\n",
        "    jnp.array([1, 2, rank - 1]), time=jnp.array([0.5, 0.75, 1.0])\n",
        ")\n",
        "print('xt_non_batch', xt_non_batch)\n",
        "print('xt_batch', xt_batch)\n",
        "assert np.all(xt_non_batch == xt_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5dq46-Ghyui"
      },
      "outputs": [],
      "source": [
        "# Test the equivallence between batched and non-batched version.\n",
        "diff_density = LinearDiffusionDensity()\n",
        "rank = 8\n",
        "q_uniform = QUniform(rank, diff_density)\n",
        "nodes = [1, 2, rank - 1]\n",
        "ts = [0.5, 0.75, 1.0]\n",
        "\n",
        "xt_lists = []\n",
        "\n",
        "for node, t in zip(nodes, ts):\n",
        "  xt_lists.append(q_uniform.exp_q(node, time=t))\n",
        "xt_non_batch = jnp.stack(xt_lists, axis=0)\n",
        "\n",
        "xt_batch = q_uniform.batch_exp_q(\n",
        "    jnp.array([1, 2, rank - 1]), time=jnp.array([0.5, 0.75, 1.0])\n",
        ")\n",
        "print('xt_non_batch', xt_non_batch)\n",
        "print('xt_batch', xt_batch)\n",
        "assert np.all(xt_non_batch == xt_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EObetPwMp79q"
      },
      "outputs": [],
      "source": [
        "# test to make sure q_row and q_col are consistent with each other for QUniform.\n",
        "diff_density = LinearDiffusionDensity()\n",
        "rank = 8\n",
        "q_uniform = QUniform(rank, diff_density)\n",
        "\n",
        "rows = []\n",
        "for i in range(rank):\n",
        "  rows.append(q_uniform.q_row(i))\n",
        "\n",
        "q_matrix = jnp.stack(rows, axis=0)\n",
        "print('q_matrix', q_matrix)\n",
        "assert np.all(q_matrix == q_uniform.q())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnDTrgKCqhTM"
      },
      "outputs": [],
      "source": [
        "# test to make sure q_row and q_col are consistent with each other for QAbsorb.\n",
        "diff_density = LinearDiffusionDensity()\n",
        "rank = 8\n",
        "q_absorb = QAbsorb(rank, diff_density)\n",
        "\n",
        "rows = []\n",
        "for i in range(rank):\n",
        "  rows.append(q_absorb.q_row(i))\n",
        "\n",
        "q_matrix = jnp.stack(rows, axis=0)\n",
        "print('q_matrix', q_matrix)\n",
        "assert np.all(q_matrix == q_absorb.q())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuOMoU-r7H69"
      },
      "outputs": [],
      "source": [
        "# test to make sure batch_q_row and q_row are consistent\n",
        "diff_density = LinearDiffusionDensity()\n",
        "rank = 8\n",
        "q_absorb = QAbsorb(rank, diff_density)\n",
        "\n",
        "rows = []\n",
        "for i in range(rank):\n",
        "  rows.append(q_absorb.q_row(i))\n",
        "\n",
        "q_matrix = jnp.stack(rows, axis=0)\n",
        "batch_q_rows = q_absorb.batch_q_row(jnp.array([i for i in range(rank)]))\n",
        "\n",
        "print('q_matrix', q_matrix)\n",
        "print('batch_q_rows', batch_q_rows)\n",
        "assert np.all(q_matrix == batch_q_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zbukx4O8AMC"
      },
      "outputs": [],
      "source": [
        "# test to make sure batch_q_row and q_row are consistent\n",
        "diff_density = LinearDiffusionDensity()\n",
        "rank = 8\n",
        "q_uniform = QUniform(rank, diff_density)\n",
        "\n",
        "rows = []\n",
        "for i in range(rank):\n",
        "  rows.append(q_uniform.q_row(i))\n",
        "\n",
        "q_matrix = jnp.stack(rows, axis=0)\n",
        "batch_q_rows = q_uniform.batch_q_row(jnp.array([i for i in range(rank)]))\n",
        "\n",
        "print('q_matrix', q_matrix)\n",
        "print('batch_q_rows', batch_q_rows)\n",
        "assert np.all(q_matrix == batch_q_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRO6KuTCKiNT"
      },
      "outputs": [],
      "source": [
        "# Test the equivallence between batched and non-batched version.\n",
        "diff_density = LinearDiffusionDensity()\n",
        "rank = 8\n",
        "q_uniform = QUniform(rank, diff_density)\n",
        "nodes = jnp.array([0, 1, 2, 3, 4, 5, 6, 7])\n",
        "ts = jnp.array([0.5] * rank)\n",
        "\n",
        "columns = q_uniform.batch_exp_q(nodes, time=ts)\n",
        "rows = q_uniform.batch_exp_q_row(nodes, time=ts)\n",
        "\n",
        "print('columns', columns)\n",
        "print('rows', rows)\n",
        "assert np.all(np.transpose(columns) == rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLq0rEe_LMWU"
      },
      "outputs": [],
      "source": [
        "# Test the equivallence between batched and non-batched version.\n",
        "diff_density = LinearDiffusionDensity()\n",
        "rank = 8\n",
        "q_absorb = QAbsorb(rank, diff_density)\n",
        "nodes = jnp.array([0, 1, 2, 3, 4, 5, 6, 7])\n",
        "ts = jnp.array([0.5] * rank)\n",
        "\n",
        "columns = q_absorb.batch_exp_q(nodes, time=ts)\n",
        "rows = q_absorb.batch_exp_q_row(nodes, time=ts)\n",
        "\n",
        "print('columns', columns)\n",
        "print('rows', rows)\n",
        "assert np.all(np.transpose(columns) == rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EB_angUGzjW"
      },
      "outputs": [],
      "source": [
        "# Test adjust_prob_ratios.\n",
        "diff_density = LinearDiffusionDensity(strength=2.0)\n",
        "rank = 8\n",
        "q_absorb = QAbsorb(rank, diff_density)\n",
        "nodes = jnp.array([0, 1, 2, 3, 4, 5, 6, 7])\n",
        "ts = jnp.array([0.5] * rank)\n",
        "\n",
        "rows = q_absorb.batch_exp_q_row(nodes, time=ts)\n",
        "print('rows', rows)\n",
        "\n",
        "p0 = jnp.array([\n",
        "    [0.1, 0.1, 0.1, 0.1, 0.2, 0.0, 0.3, 0.1],\n",
        "    [0.1, 0.2, 0.1, 0.1, 0.1, 0.0, 0.3, 0.1],\n",
        "    ])\n",
        "\n",
        "p1 = jnp.matmul(rows, jnp.transpose(p0))\n",
        "p1 = jnp.transpose(p1)\n",
        "\n",
        "print('p1', p1)\n",
        "print(jnp.sum(p1, axis=1))\n",
        "\n",
        "p0_inversed = q_absorb.adjust_prob_ratio(p1, jnp.array([0.5, 0.5]))\n",
        "print('p0_inversed', p0_inversed)\n",
        "\n",
        "print('delta p0 and p0_inversed', jnp.abs(p0 - p0_inversed))\n",
        "assert np.all(np.abs(p0 - p0_inversed) \u003c 2e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Hwe-fkc5g_D"
      },
      "outputs": [],
      "source": [
        "# Test adjust_prob_ratios.\n",
        "diff_density = LinearDiffusionDensity(strength=2.0)\n",
        "rank = 8\n",
        "q_uniform = QUniform(rank, diff_density)\n",
        "nodes = jnp.array([0, 1, 2, 3, 4, 5, 6, 7])\n",
        "ts = jnp.array([0.7] * rank)\n",
        "\n",
        "rows = q_uniform.batch_exp_q_row(nodes, time=ts)\n",
        "print('rows', rows)\n",
        "\n",
        "p0 = jnp.array([\n",
        "    [0.1, 0.1, 0.1, 0.1, 0.2, 0.0, 0.3, 0.1],\n",
        "    [0.1, 0.2, 0.1, 0.1, 0.1, 0.0, 0.3, 0.1],\n",
        "    ])\n",
        "\n",
        "p1 = jnp.matmul(rows, jnp.transpose(p0))\n",
        "p1 = jnp.transpose(p1)\n",
        "\n",
        "print('p1', p1)\n",
        "print(jnp.sum(p1, axis=1))\n",
        "p0_inversed = q_uniform.adjust_prob_ratio(p1, jnp.array([0.7, 0.7]))\n",
        "print('p0_inversed', p0_inversed)\n",
        "\n",
        "print('delta p0 and p0_inversed', jnp.abs(p0 - p0_inversed))\n",
        "assert np.all(np.abs(p0 - p0_inversed) \u003c 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOmy6wnUqjS9"
      },
      "source": [
        "# Algorithm pieces needed for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mVCbpVpBWzM"
      },
      "outputs": [],
      "source": [
        "def _gumbel_max_sample(prob_ts, random_seed):\n",
        "  \"\"\"Samples a token per batch element using the gumbel-max trick.\"\"\"\n",
        "  assert prob_ts.ndim == 2\n",
        "  # use the gumble-max trick to sample from the categorical distribution.\n",
        "  min_log_prob = -100.0  # exp(-100.0) is small enough to be ignored.\n",
        "  # Note(yonghui): maybe we don't need to replace -inf by -100.0\n",
        "  log_prob_ts = jnp.nan_to_num(\n",
        "      jnp.log(prob_ts), nan=min_log_prob, neginf=min_log_prob\n",
        "  )\n",
        "  noise = jax.random.gumbel(random_seed, shape=log_prob_ts.shape)\n",
        "  samples = jnp.argmax(log_prob_ts + noise, axis=-1, keepdims=True)\n",
        "  sampled_log_probs = jnp.take_along_axis(log_prob_ts, samples, axis=-1)\n",
        "  log_prob_ratio = log_prob_ts - sampled_log_probs\n",
        "  samples = jnp.squeeze(samples, axis=-1)\n",
        "  return samples, log_prob_ratio\n",
        "\n",
        "\n",
        "def _forced_sample(prob_ts, samples):\n",
        "  \"\"\"Compute log-prob ration with forced samples.\"\"\"\n",
        "  assert prob_ts.ndim == 2\n",
        "  assert samples.ndim == 1\n",
        "\n",
        "  # use the gumble-max trick to sample from the categorical distribution.\n",
        "  min_log_prob = -100.0  # exp(-100.0) is small enough to be ignored.\n",
        "  # Note(yonghui): maybe we don't need to replace -inf by -100.0\n",
        "  log_prob_ts = jnp.nan_to_num(\n",
        "      jnp.log(prob_ts), nan=min_log_prob, neginf=min_log_prob\n",
        "  )\n",
        "  sampled_log_probs = jnp.take_along_axis(\n",
        "      log_prob_ts, samples[:, jnp.newaxis], axis=-1\n",
        "  )\n",
        "  log_prob_ratio = log_prob_ts - sampled_log_probs\n",
        "  return samples, log_prob_ratio\n",
        "\n",
        "\n",
        "# Samples nosified examples and the groundtruth prob ratio\n",
        "#\n",
        "# Please refer to algo-1 in the paper https://arxiv.org/pdf/2310.16834 for\n",
        "# details.\n",
        "def DiffuseAndSampleXt(\n",
        "    x0: Int32[Array, 'B T'],\n",
        "    ts: Float[Array, 'B'],\n",
        "    transition_matrix: TransitionMatrix,\n",
        "    random_seed: jax.random.PRNGKey,\n",
        "    forced_samples: Int32[Array, 'B T'] | None = None,\n",
        "):\n",
        "  \"\"\"Given x0, sample nosified examples at time t.\n",
        "\n",
        "  Args:\n",
        "    x0: the input batch, of shape [B, T]. Here we assume x0 is not packed. Each\n",
        "      element contains one single example.\n",
        "    ts: times at which to sample X_t from.\n",
        "    transition_matrix: the transition matrix.\n",
        "    random_seed: the random seed.\n",
        "    forced_samples: the forced samples. If they are present, this function only\n",
        "      computes the log-prob ratio given the forced samples. This is mostly\n",
        "      useful for testing purposes.\n",
        "\n",
        "  Returns:\n",
        "    A tuple (noisified_example, log_prob_ratio), where noisified_example is the\n",
        "      noisified example at time ts, and log_prob_ratio is the log probability\n",
        "      ratio between all other tokens and the sampled tokens at time ts.\n",
        "      noisified_example is of shape [B, T], and prob_ratio is of shape\n",
        "      [B, T, N].\n",
        "  \"\"\"\n",
        "  batch_size = x0.shape[0]\n",
        "  seq_length = x0.shape[1]\n",
        "  assert ts.ndim == 1\n",
        "  assert ts.shape[0] == batch_size\n",
        "  ts_replicated = jnp.repeat(ts[:, jnp.newaxis], seq_length, axis=1)\n",
        "\n",
        "  nodes_1d = jnp.reshape(x0, [-1])\n",
        "  ts_1d = jnp.reshape(ts_replicated, [-1])\n",
        "  # prob_ts is the probability distribution at time t for all the nodes\n",
        "  prob_ts = transition_matrix.batch_exp_q(nodes_1d, ts_1d)\n",
        "  if forced_samples is not None:\n",
        "    assert forced_samples.shape == x0.shape\n",
        "    forced_samples = jnp.reshape(forced_samples, [-1])\n",
        "    samples, log_prob_ratio = _forced_sample(prob_ts, forced_samples)\n",
        "  else:\n",
        "    samples, log_prob_ratio = _gumbel_max_sample(prob_ts, random_seed)\n",
        "  samples = jnp.reshape(samples, [batch_size, seq_length])\n",
        "  log_prob_ratio = jnp.reshape(log_prob_ratio, [batch_size, seq_length, -1])\n",
        "  return samples, log_prob_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJnrMSngE0Wr"
      },
      "outputs": [],
      "source": [
        "prob_ts = jnp.array([[0.1, 0.2, 0.3, 0.4], [0.25, 0, 0.25, 0.5]])\n",
        "\n",
        "freq = np.zeros_like(prob_ts)\n",
        "num_trials = 1000\n",
        "\n",
        "for i in range(num_trials):\n",
        "  s, x = _gumbel_max_sample(prob_ts, random_seed=jax.random.PRNGKey(i))\n",
        "  for row_i, row in enumerate(s):\n",
        "    freq[row_i, row] += 1\n",
        "\n",
        "print(freq / num_trials)\n",
        "\n",
        "s, x = _gumbel_max_sample(prob_ts, random_seed=jax.random.PRNGKey(100))\n",
        "print(s)\n",
        "print(jnp.exp(x))\n",
        "\n",
        "s, x_new = _forced_sample(prob_ts, s)\n",
        "print(s)\n",
        "print(jnp.exp(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b2himAI2UbNd"
      },
      "outputs": [],
      "source": [
        "# Test the implementation of LinearDiffusionDensity.\n",
        "vocab_size = 16\n",
        "diff_density = LinearDiffusionDensity(strength=2.0)\n",
        "q_absorb = QAbsorb(vocab_size, diff_density)\n",
        "q_uniform = QUniform(vocab_size, diff_density)\n",
        "\n",
        "x0 = jnp.array([[1, 2], [3, 4]])\n",
        "ts = jnp.array([0.1, 1.0])\n",
        "\n",
        "sample, log_prob_ratio = DiffuseAndSampleXt(x0, ts, q_absorb,\n",
        "                                            jax.random.PRNGKey(100))\n",
        "print(sample)\n",
        "print(log_prob_ratio)\n",
        "\n",
        "sample, log_prob_ratio = DiffuseAndSampleXt(x0, ts, q_uniform,\n",
        "                                            jax.random.PRNGKey(100))\n",
        "print(sample)\n",
        "print(log_prob_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s04UJ6AUi-tM"
      },
      "outputs": [],
      "source": [
        "# This is the loss function used in SEDD.\n",
        "def compute_score_entropy(\n",
        "    sigma_t: Float[Array, 'B T'],\n",
        "    noisified_samples: Int32[Array, 'B T'],\n",
        "    valid_tokens: Float[Array, 'B T'],\n",
        "    log_prob_ratio: Float[Array, 'B T N'],\n",
        "    predicted_log_prob_ratio: Float[Array, 'B T N'],\n",
        "):\n",
        "  \"\"\"Computes the score entropy loss.\n",
        "\n",
        "  In a real training loop, noisified_samples is the input the transformer block.\n",
        "  In addition to taking noisified_samples as input, the transformer block also\n",
        "  takes ts (time at each token) as input.\n",
        "  The transformer block predicts predicted_log_prob_ratio for each possible\n",
        "  token in the vocab. It is basically the linear output before the softmax\n",
        "  layer.\n",
        "\n",
        "  Args:\n",
        "    sigma_t: the diffusion strength at time t.\n",
        "    noisified_samples: the noisified samples at time t.\n",
        "    valid_tokens: a mask of valid tokens. It is 0.0 if the token at the pos is\n",
        "      invalid (e.g. padded toke), and it is 1.0 otherwise.\n",
        "    log_prob_ratio: the log probability ratio between all other tokens and the\n",
        "      nosified tokens at time t.\n",
        "    predicted_log_prob_ratio: the predicted log probability ratio between all\n",
        "      other tokens and the noisified tokens at time t.\n",
        "\n",
        "  Returns:\n",
        "    The score entropy loss, and auxilary info that can be useful for debugging.\n",
        "  \"\"\"\n",
        "  vocab_size = log_prob_ratio.shape[-1]\n",
        "  # Indices of the nosified examples at time t\n",
        "  xt_indices = jax.nn.one_hot(noisified_samples, vocab_size)\n",
        "  # compute the score entropy loss.\n",
        "  # The loss is minimized if predicted_log_prob_ratio and the groundtruth\n",
        "  # ratio are the same.\n",
        "  #\n",
        "  # TODO(yonghui): normalize the loss such that it is non-negative.\n",
        "  assert predicted_log_prob_ratio.shape == log_prob_ratio.shape\n",
        "  loss = (\n",
        "      jnp.exp(predicted_log_prob_ratio)\n",
        "      - jnp.exp(log_prob_ratio) * predicted_log_prob_ratio\n",
        "  )\n",
        "  # normalization constant\n",
        "  k_norm = (\n",
        "      jnp.exp(log_prob_ratio)\n",
        "      - jnp.exp(log_prob_ratio) * log_prob_ratio\n",
        "  )\n",
        "  loss = loss - k_norm\n",
        "  # mask out loss on tokens in x_t. loss on those tokens should be 0.0 anyways.\n",
        "  loss = (1.0 - xt_indices) * loss\n",
        "  per_token_loss = jnp.sum(loss, -1)\n",
        "  assert sigma_t.shape == per_token_loss.shape == valid_tokens.shape\n",
        "  per_sequence_loss = sigma_t * per_token_loss * valid_tokens\n",
        "  summaries = {\n",
        "      'per_token_element_loss': loss,\n",
        "      'per_token_loss': per_token_loss,\n",
        "      'per_sequence_loss': per_sequence_loss,\n",
        "      'nosied_samples': noisified_samples,\n",
        "      'sigma_t': sigma_t,\n",
        "  }\n",
        "  num_valid_tokens = jnp.sum(valid_tokens)\n",
        "  final_loss = jnp.sum(per_sequence_loss) / num_valid_tokens\n",
        "  return final_loss, summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfiPxQYQ_QvY"
      },
      "source": [
        "## Example implementation of the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWcx7zogp74h"
      },
      "outputs": [],
      "source": [
        "# Test out the loss function\n",
        "vocab_size = 16\n",
        "diff_density = LinearDiffusionDensity(strength=2.0)\n",
        "q_uniform = QUniform(vocab_size, diff_density)\n",
        "\n",
        "x0 = jnp.array([[1, 2, 3], [4, 5, 6]])\n",
        "ts = jnp.array([0.1, 0.5])\n",
        "\n",
        "samples, log_prob_ratio = DiffuseAndSampleXt(\n",
        "    x0, ts, q_uniform, jax.random.PRNGKey(100)\n",
        ")\n",
        "\n",
        "# The training algorithm will produce an estimate log_prob_ratio.\n",
        "# Here, we simulate training by producing the estimated log_prob_ratio\n",
        "# from a slightly different time.\n",
        "ts_prime = jnp.array([0.2, 0.4])\n",
        "_, estimated_prob_ratio = DiffuseAndSampleXt(\n",
        "    x0, ts_prime, q_uniform, jax.random.PRNGKey(100), forced_samples=samples\n",
        ")\n",
        "\n",
        "print('x0', x0)\n",
        "print('ts', ts)\n",
        "print('samples', samples)\n",
        "print('log_prob_ratio', log_prob_ratio)\n",
        "print('estimated_prob_ratio', estimated_prob_ratio)\n",
        "\n",
        "seq_len = samples.shape[1]\n",
        "sigma_t = diff_density.sigma_t(ts)\n",
        "sigma_t = jnp.repeat(sigma_t[:, jnp.newaxis], seq_len, axis=1)\n",
        "\n",
        "valid_tokens = jnp.ones_like(samples)\n",
        "\n",
        "print('log_prob_ratio', log_prob_ratio)\n",
        "print('estimated_prob_ratio', estimated_prob_ratio)\n",
        "\n",
        "final_loss, summaries = compute_score_entropy(\n",
        "    sigma_t,\n",
        "    samples,\n",
        "    valid_tokens,\n",
        "    log_prob_ratio,\n",
        "    predicted_log_prob_ratio=estimated_prob_ratio,\n",
        ")\n",
        "\n",
        "print(final_loss)\n",
        "print(summaries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko_Dspapqqs5"
      },
      "source": [
        "# Algorithm pieces needed for sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNzyi3t5IY_Z"
      },
      "outputs": [],
      "source": [
        "# First test the implementation of q_posterior\n",
        "vocab_size = 16\n",
        "diff_density = LinearDiffusionDensity(strength=2.0)\n",
        "q_absorb = QAbsorb(vocab_size, diff_density)\n",
        "q_uniform = QUniform(vocab_size, diff_density)\n",
        "\n",
        "x0 = jnp.array([[1, 2], [3, 4]])\n",
        "ts = jnp.array([0.1, 1.0])\n",
        "\n",
        "print('Test absorb transition matrix')\n",
        "sample, log_prob_ratio = DiffuseAndSampleXt(\n",
        "    x0, ts, q_absorb, jax.random.PRNGKey(100)\n",
        ")\n",
        "print('sample', sample)\n",
        "print('log_prob_ratio', log_prob_ratio)\n",
        "\n",
        "# Now compute the posterior probs.\n",
        "sample_1d = jnp.reshape(sample, [-1])\n",
        "log_prob_ratio_1d = jnp.reshape(log_prob_ratio, [sample_1d.shape[0], -1])\n",
        "posterior_probs = q_absorb.q_posterior(sample_1d, jnp.exp(log_prob_ratio_1d))\n",
        "print('q_absorb posterior_probs', posterior_probs)\n",
        "# make sure posterior_probs sum up to 0\n",
        "print('posterior_probs sum', jnp.sum(posterior_probs, -1))\n",
        "assert jnp.all(jnp.abs(jnp.sum(posterior_probs, -1)) \u003c 1e-6)\n",
        "\n",
        "print('Test uniform transition matrix')\n",
        "sample, log_prob_ratio = DiffuseAndSampleXt(\n",
        "    x0, ts, q_uniform, jax.random.PRNGKey(100)\n",
        ")\n",
        "print('sample', sample)\n",
        "print('log_prob_ratio', log_prob_ratio)\n",
        "\n",
        "# Now compute the posterior probs.\n",
        "sample_1d = jnp.reshape(sample, [-1])\n",
        "log_prob_ratio_1d = jnp.reshape(log_prob_ratio, [sample_1d.shape[0], -1])\n",
        "posterior_probs = q_uniform.q_posterior(sample_1d, jnp.exp(log_prob_ratio_1d))\n",
        "print('q_uniform posterior_probs', posterior_probs)\n",
        "\n",
        "# make sure posterior_probs sum up to 0\n",
        "print('posterior_probs sum', jnp.sum(posterior_probs, -1))\n",
        "assert jnp.all(jnp.abs(jnp.sum(posterior_probs, -1)) \u003c 1e-6)\n",
        "\n",
        "# TODO(yonghui): Add a bunch more tests to assert the validity of the\n",
        "# impementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Te8Q9SZNxey"
      },
      "source": [
        "## Example implementation of the euler sampling algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1Uif892quEK"
      },
      "outputs": [],
      "source": [
        "# An example implementation of the Euler sampling algorithm\n",
        "def TestEulerSampler(q_matrix_type: str):\n",
        "  # Now we implement the euler sampling algorithm.\n",
        "  vocab_size = 16\n",
        "\n",
        "  # jnp.exp(-10) is small enough that any initial distribution will be diffused\n",
        "  # into complete noise.\n",
        "  diff_density = LinearDiffusionDensity(strength=4.0)\n",
        "\n",
        "  if q_matrix_type == 'absorb':\n",
        "    # First test out the uniform diffusion matrix.\n",
        "    q_matrix = QAbsorb(vocab_size, diff_density)\n",
        "  elif q_matrix_type == 'uniform':\n",
        "    q_matrix = QUniform(vocab_size, diff_density)\n",
        "  else:\n",
        "    raise ValueError(f'Unknown q_matrix_type {q_matrix_type}')\n",
        "\n",
        "  # This is the state that we would like to recover from.\n",
        "  x0 = jnp.array([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "  samples, _ = DiffuseAndSampleXt(\n",
        "      x0, jnp.array([1.0, 1.0]), q_matrix, jax.random.PRNGKey(100)\n",
        "  )\n",
        "\n",
        "  # samples should be complete noise\n",
        "  print('samples', samples)\n",
        "\n",
        "  def estimate_prob_ratio(ts, xt, x0=x0):\n",
        "    # Estimate the prob ratio at times 'ts' conditioned on x_t being \"xt\".\n",
        "    # Here we cheat by providing the groundtruth prob ratios.\n",
        "    # In the real sampling algorithm, the prob ratio should come from our model\n",
        "    # predictions.\n",
        "    assert xt.shape == x0.shape\n",
        "    assert ts.ndim == 1\n",
        "    assert ts.shape[0] == xt.shape[0]\n",
        "    _, estimated_prob_ratio = DiffuseAndSampleXt(\n",
        "        x0, ts, q_matrix, None, forced_samples=xt\n",
        "    )\n",
        "    return jnp.exp(estimated_prob_ratio)\n",
        "\n",
        "  # We should test out much smaller step sizes\n",
        "  num_iterations = 100\n",
        "  step_size = 1.0 / num_iterations\n",
        "\n",
        "  xt = samples\n",
        "  batch_size = samples.shape[0]\n",
        "\n",
        "  # Here is the sampling loop.\n",
        "  for i in range(num_iterations, 0, -1):\n",
        "    t_begin = (i - 1) * step_size\n",
        "    t_end = i * step_size\n",
        "    t_delta = diff_density.cum_sigma_t(t_end) - diff_density.cum_sigma_t(\n",
        "        t_begin\n",
        "    )\n",
        "    prob_ratio = estimate_prob_ratio(jnp.zeros([batch_size]) + t_end, xt)\n",
        "\n",
        "    xt_1d = jnp.reshape(xt, [-1])\n",
        "    prob_ratio_1d = jnp.reshape(prob_ratio, [xt_1d.shape[0], -1])\n",
        "    posterior = q_matrix.q_posterior(xt_1d, prob_ratio_1d)\n",
        "    xt_one_hot = jax.nn.one_hot(xt_1d, vocab_size)\n",
        "    # Take one step backward in time.\n",
        "    # P(x_{t - \\delta t} | x_t =\n",
        "    #        p(x_t) +\n",
        "    #        t_delta * posterior(x_{t - \\delta t}, x_t))\n",
        "    #\n",
        "    # This is the differential equation for the backward process.\n",
        "    xt_minus_delta = xt_one_hot + t_delta * posterior\n",
        "    # Now we sample the new xt\n",
        "    xt_minus_delta_sample, _ = _gumbel_max_sample(\n",
        "        xt_minus_delta, jax.random.PRNGKey(100 + i)\n",
        "    )\n",
        "    xt = jnp.reshape(xt_minus_delta_sample, xt.shape)\n",
        "    print('xt', xt)\n",
        "\n",
        "  x_final = xt\n",
        "\n",
        "  # Here we assert we fully recover the original x0\n",
        "  assert jnp.all(x_final == x0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncLp9PGeaZqq"
      },
      "outputs": [],
      "source": [
        "TestEulerSampler(q_matrix_type = 'absorb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0COOdsyadIY"
      },
      "outputs": [],
      "source": [
        "TestEulerSampler(q_matrix_type = 'uniform')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a51TotPCPQJe"
      },
      "outputs": [],
      "source": [
        "# An example implementation of the Tweedie sampling algorithm\n",
        "#\n",
        "# Note, this is not a full implementation of the algorithm. In particular, here\n",
        "# we are missing the part to adjust the prob ratio based on Exp({-\\delta t} * Q)\n",
        "def TestTweedieSampler(q_matrix_type: str):\n",
        "  # Now we implement the euler sampling algorithm.\n",
        "  vocab_size = 16\n",
        "\n",
        "  # jnp.exp(-10) is small enough that any initial distribution will be diffused\n",
        "  # into complete noise.\n",
        "  diff_density = LinearDiffusionDensity(strength=4.0)\n",
        "\n",
        "  if q_matrix_type == 'absorb':\n",
        "    # First test out the uniform diffusion matrix.\n",
        "    q_matrix = QAbsorb(vocab_size, diff_density)\n",
        "  elif q_matrix_type == 'uniform':\n",
        "    q_matrix = QUniform(vocab_size, diff_density)\n",
        "  else:\n",
        "    raise ValueError(f'Unknown q_matrix_type {q_matrix_type}')\n",
        "\n",
        "  # This is the state that we would like to recover from.\n",
        "  x0 = jnp.array([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "  samples, _ = DiffuseAndSampleXt(\n",
        "      x0, jnp.array([1.0, 1.0]), q_matrix, jax.random.PRNGKey(100)\n",
        "  )\n",
        "\n",
        "  # samples should be complete noise\n",
        "  print('samples', samples)\n",
        "\n",
        "  def estimate_prob_ratio(ts, xt, x0=x0):\n",
        "    # Estimate the prob ratio at times 'ts' conditioned on x_t being \"xt\".\n",
        "    # Here we cheat by providing the groundtruth prob ratios.\n",
        "    # In the real sampling algorithm, the prob ratio should come from our model\n",
        "    # predictions.\n",
        "    assert xt.shape == x0.shape\n",
        "    assert ts.ndim == 1\n",
        "    assert ts.shape[0] == xt.shape[0]\n",
        "    _, estimated_prob_ratio = DiffuseAndSampleXt(\n",
        "        x0, ts, q_matrix, None, forced_samples=xt\n",
        "    )\n",
        "    return jnp.exp(estimated_prob_ratio)\n",
        "\n",
        "  # We should test out much smaller step sizes\n",
        "  num_iterations = 100\n",
        "  step_size = 1.0 / num_iterations\n",
        "\n",
        "  xt = samples\n",
        "  batch_size = samples.shape[0]\n",
        "  num_tokens = samples.shape[0] * samples.shape[1]\n",
        "\n",
        "  # Here is the sampling loop.\n",
        "  for i in range(num_iterations, 0, -1):\n",
        "    t_begin = (i - 1) * step_size\n",
        "    t_end = i * step_size\n",
        "    t_delta = diff_density.cum_sigma_t(t_end) - diff_density.cum_sigma_t(\n",
        "        t_begin\n",
        "    )\n",
        "    prob_ratio = estimate_prob_ratio(jnp.zeros([batch_size]) + t_end, xt)\n",
        "\n",
        "    xt_1d = jnp.reshape(xt, [-1])\n",
        "    prob_ratio_1d = jnp.reshape(prob_ratio, [xt_1d.shape[0], -1])\n",
        "    batch_t_delta = jnp.ones([xt_1d.shape[0]]) * t_delta\n",
        "\n",
        "    # adjust the prob ratio, basically, this takes care of this part\n",
        "    # exp(-tQ) part in equation 18.\n",
        "    prob_ratio_1d_adjusted = q_matrix.adjust_prob_ratio(\n",
        "        prob_ratio_1d, jnp.zeros([num_tokens]) + step_size\n",
        "    )\n",
        "\n",
        "    posterior = (\n",
        "        q_matrix.batch_exp_q_row(xt_1d, batch_t_delta) * prob_ratio_1d_adjusted\n",
        "    )\n",
        "    posterior_sum = jnp.sum(posterior, axis=-1, keepdims=True)\n",
        "    normed_posterior = posterior / posterior_sum\n",
        "\n",
        "    # Now we sample the new xt\n",
        "    xt_minus_delta_sample, _ = _gumbel_max_sample(\n",
        "        normed_posterior, jax.random.PRNGKey(100 + i)\n",
        "    )\n",
        "    xt = jnp.reshape(xt_minus_delta_sample, xt.shape)\n",
        "    print('xt', xt)\n",
        "\n",
        "  x_final = xt\n",
        "\n",
        "  # Here we assert we fully recover the original x0\n",
        "  assert jnp.all(x_final == x0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ8O1gdgUWbV"
      },
      "outputs": [],
      "source": [
        "TestTweedieSampler(q_matrix_type = 'absorb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_w5XSVC_j8M"
      },
      "outputs": [],
      "source": [
        "TestTweedieSampler(q_matrix_type = 'uniform')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3_tpu",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1xKmTDdHBaJ1BjUUZndEHs7R0z48YQPWp",
          "timestamp": 1722294055976
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
